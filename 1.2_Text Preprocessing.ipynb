{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Apple is looking at buying U.K. startup for $1 billion\n",
      "\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# testing spacy library\n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print('Original Sentence: {}'.format(sentence))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -  Removing Stopwards & Text Normalization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp  = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "# 'nlp' object is used to create documents with Linguistic annotations.\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of word tokens\n",
    "word_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    word_tokens.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original Sentence \n",
      "\n",
      "\n",
      "['He', 'determined', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', 'his', 'claims', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', 'He', 'was', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'rights', 'had', 'become', 'much', 'less', 'valuable', ',', 'and', 'he', 'had', '\\n', 'indeed', 'the', 'vaguest', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were', '.']\n",
      "\n",
      "\n",
      "Filtered Sentence \n",
      "\n",
      "\n",
      "['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "# create a list of word tokens after removing stopwords\n",
    "filtered_sentence = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word)\n",
    "\n",
    "print(\"\\n\\nOriginal Sentence \\n\\n\")\n",
    "print(word_tokens)\n",
    "\n",
    "print(\"\\n\\nFiltered Sentence \\n\\n\")\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Stopword removal doesn't take off punctuation marks or newline character. Need to remove them manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Stemming - spaCy has no module for stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'determine', 'to', 'drop', '-PRON-', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', '-PRON-', 'claim', 'to', 'the', 'wood', '-', 'cut', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', '-PRON-', 'be', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'right', 'have', 'become', 'much', 'less', 'valuable', ',', 'and', '-PRON-', 'have', '\\n', 'indeed', 'the', 'vague', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'be', '.']\n"
     ]
    }
   ],
   "source": [
    "lemma_word = []\n",
    "for token in doc:\n",
    "    lemma_word.append(token.lemma_)\n",
    "\n",
    "print(lemma_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'determine', 'to', 'drop', '-PRON-', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', '-PRON-', 'claim', 'to', 'the', 'wood', '-', 'cut', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', '-PRON-', 'be', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'right', 'have', 'become', 'much', 'less', 'valuable', ',', 'and', '-PRON-', 'have', '\\n', 'indeed', 'the', 'vague', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'be', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
    "\n",
    "lemma_word1 = [] \n",
    "for token in doc:\n",
    "    lemma_word1.append(token.lemma_)\n",
    "print(lemma_word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK version: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitchaudhary/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "print('NLTK version: {}'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Apple is looking at buying U.K. startup for $1 billion\n",
      "\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# testing nltk library\n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print('Original Sentence: {}'.format(sentence))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - Removing Stopwards & Text Normalization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ankitchaudhary/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentence\n",
    "text = \"\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens of words\n",
    "word_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original Sentence \n",
      "\n",
      "\n",
      "He determined to drop his litigation with the monastry , and relinguish his claims to the wood - cuting and \n",
      " fishery rihgts at once . He was the more ready to do this becuase the rights had become much less valuable , and he had \n",
      " indeed the vaguest idea where the wood and river in question were .\n",
      "\n",
      "\n",
      "Filtered Sentence \n",
      "\n",
      "\n",
      "He determined drop litigation monastry , relinguish claims wood - cuting \n",
      " fishery rihgts . He ready becuase rights become much less valuable , \n",
      " indeed vaguest idea wood river question .\n"
     ]
    }
   ],
   "source": [
    "# create a list of word tokens after removing stopwords\n",
    "filtered_sentence = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_sentence.append(word)\n",
    "\n",
    "print(\"\\n\\nOriginal Sentence \\n\\n\")\n",
    "print(\" \".join(word_tokens)) \n",
    "\n",
    "print(\"\\n\\nFiltered Sentence \\n\\n\")\n",
    "print(\" \".join(filtered_sentence)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see that the size of the text has almost reduced to half!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Filtered Sentence \n",
      "\n",
      " ['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "\n",
      "\n",
      "Sentence with Stemming\n",
      "\n",
      " ['determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood', '-', 'cute', '\\n', 'fisheri', 'rihgt', '.', 'readi', 'becuas', 'right', 'valuabl', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "stem_words = []\n",
    "for word in filtered_sentence:\n",
    "    root_word = ps.stem(word)\n",
    "    stem_words.append(root_word)\n",
    "\n",
    "print(\"\\n\\nFiltered Sentence \\n\\n\",filtered_sentence)\n",
    "print(\"\\n\\nSentence with Stemming\\n\\n\",stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ankitchaudhary/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Filtered Sentence \n",
      "\n",
      " ['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "\n",
      "\n",
      "Sentence with Lemmatization\n",
      "\n",
      " ['determine', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claim', 'wood', '-', 'cut', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'right', 'valuable', ',', '\\n', 'vague', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "lemma_word = []\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    word = wordnet_lem.lemmatize(word, pos = 'n')\n",
    "    word = wordnet_lem.lemmatize(word, pos = 'v')\n",
    "    word = wordnet_lem.lemmatize(word, pos = 'a')\n",
    "    lemma_word.append(word)\n",
    "\n",
    "print(\"\\n\\nFiltered Sentence \\n\\n\",filtered_sentence)\n",
    "print(\"\\n\\nSentence with Lemmatization\\n\\n\",lemma_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The lemmatizer only lemmatizes those words which match the pos parameter of the lemmatize method. \n",
    "- Lemmatization is done on the basis of part-of-speech tagging (POS tagging). We’ll talk in detail about POS tagging in an upcoming article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
